{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scrap Lengkap.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5iT2jCcjvmW"
      },
      "source": [
        "#Install Library First\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2ipWRYzjsf9"
      },
      "source": [
        "#import library\n",
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "#copy paste NPSN sekolah dari .csv (run cloud)\n",
        "list_npsn = [20219178,20219146,20219146,20219145,20219144,20219143,20219142,20219141,20219140,20219139,20219176,20219175,20219174,20219161,20219160,20219159,20219137,20219148,20219158,20265543,20219295]\n",
        "\n",
        "#Make Array for exporting csv\n",
        "data_export = []\n",
        "\n",
        "### Fungsi untuk cari sekolah berdasarkan NPSN\n",
        "# input NPSN tipe integer\n",
        "# output Nama sekolah dan link sekolah dalam bentuk tupple\n",
        "\n",
        "def find_url_and_school_name(npsn):\n",
        "    headers = {\n",
        "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36 Edg/89.0.774.76'\n",
        "    }\n",
        "    payload = {\n",
        "        'page':1,\n",
        "        'kode_kabupaten':'',\n",
        "        'kode_kecamatan':'',\n",
        "        'bentuk_pendidikan':'',\n",
        "        'status_sekolah':'semua',\n",
        "        'nama':npsn\n",
        "        }\n",
        "    try:\n",
        "      session = requests.Session()\n",
        "      response = session.post('http://sekolah.data.kemdikbud.go.id/index.php/Chome/pencarian/',headers=headers,data=payload)\n",
        "      soup = bs(response.content,'html.parser')\n",
        "      a = soup.find('ul',attrs={'class':'list-group list-group-unbordered'}).find('li',attrs={'class':'list-group-item'}).find('a')\n",
        "      nama_sekolah = a.text\n",
        "      link_sekolah = a.attrs['href']\n",
        "    except:\n",
        "      print(\"ada yang error bro\")\n",
        "    return nama_sekolah,link_sekolah\n",
        "\n",
        "#array untuk data sekolah yang akan di scrap\n",
        "data_sekolah = []\n",
        "error_log = []\n",
        "\n",
        "#Get All school list and Link\n",
        "for no_npsn in list_npsn:\n",
        "  for l in range(0,2):\n",
        "    data_sekolah.append(find_url_and_school_name(no_npsn)[l])\n",
        "\n",
        "#Initial Value\n",
        "print(data_sekolah[0])\n",
        "f = 0\n",
        "e = 0\n",
        "error_message = 'Data Tidak Ditemukan'\n",
        "\n",
        "#Scrapping per School\n",
        "for a in range(1,len(data_sekolah)+1,2):\n",
        "  #Get url\n",
        "  res = requests.get(data_sekolah[a])\n",
        "\n",
        "  #Make object\n",
        "  soup = bs(res.content)\n",
        "\n",
        "  #Array fix\n",
        "  data_fix = []\n",
        "\n",
        "  #Input Nama Sekolah\n",
        "  data_fix.append(data_sekolah[e])\n",
        "  e = e+2\n",
        "\n",
        "  #Get Kepala Sekolah and Operator\n",
        "  data = soup.find_all('li', class_=\"list-group-item\")\n",
        "  data2 = []\n",
        " \n",
        "  for i in data:\n",
        "    data2.append(str(i.text))\n",
        "    \n",
        "  print(data2)\n",
        "  #if \"Kepala Sekolah\" and \"Operator\" in data2:\n",
        "  #Processing String\n",
        "  col1 = data2[2]\n",
        "  col2 = data2[3]\n",
        "  col1.lstrip()\n",
        "  col2.lstrip()\n",
        "  kepsek = col1[19:]\n",
        "  operator = col2[13:]  \n",
        "\n",
        "  #Input kepsek dan operator ke data_fix\n",
        "  data_fix.append(kepsek)\n",
        "  data_fix.append(operator)\n",
        "\n",
        "  # else :\n",
        "  #   error_log.append(\"NULL\")\n",
        "  #   data_fix.append(error_message)\n",
        "  #   data_fix.append(error_message)\n",
        "\n",
        "  #Search Siswa dan Rombel\n",
        "  array = []\n",
        "  for id in soup.find_all(id=\"siswatingkat\"):\n",
        "    for td in id.find_all('td'):\n",
        "      array.append(str(td.text))\n",
        "\n",
        "  #Input Data Jumlah Siswa (urut kelas 10 hingga 12)\n",
        "  for index in range(5,10,2):\n",
        "    data_fix.append(array[index])\n",
        "\n",
        "  #Input Data Rombel Kelas (urut kelas 10 hingga 12)\n",
        "  for index2 in range(17,23,2):\n",
        "    data_fix.append(array[index2])\n",
        "\n",
        "  #Jumlah Siswa Laki-laki dan Perempuan\n",
        "  data = soup.find_all('font',class_='text-info')\n",
        "  data2 = []\n",
        "  for i in data:\n",
        "    data2.append(str(i.text))\n",
        "\n",
        "  #Input Siswa Laki-laki dan Perempuan\n",
        "  siswa_male = data2[1]\n",
        "  data_fix.append(siswa_male)\n",
        "  siswa_female = data2[2]\n",
        "  data_fix.append(siswa_female)\n",
        "\n",
        "  #Input Data Semester\n",
        "  semester_data = data2[5]\n",
        "  data_fix.append(semester_data)\n",
        "\n",
        "  #Data Akreditasi\n",
        "  data = soup.find_all('li',class_='list-group-item-info')\n",
        "  data2 = []\n",
        "  for i in data:\n",
        "    data2.append(str(i.text))\n",
        "  akreditasi = data2[0]  \n",
        "\n",
        "  #Input data Akreditasi\n",
        "  akreditasi = akreditasi[-1]\n",
        "  data_fix.append(akreditasi)\n",
        "\n",
        "  #Data Alamat\n",
        "  data = soup.find_all('font',class_='small')\n",
        "  data2 = []\n",
        "  for i in data:\n",
        "    data2.append(str(i.text))\n",
        "  alamat = data2[0]\n",
        "\n",
        "  #Input data Alamat\n",
        "  alamat = alamat[:-19]\n",
        "  data_fix.append(alamat)\n",
        "\n",
        "  #Data Email Website\n",
        "  try:\n",
        "    b = f\"https://referensi.data.kemdikbud.go.id/tabs.php?npsn={list_npsn[f]}\"\n",
        "    f = f+1\n",
        "    res2 = requests.get(b)\n",
        "    soup2 = bs(res2.content,'html.parser')\n",
        "    array = [] \n",
        "    email = [] \n",
        "    website = []\n",
        "    sub_mail = '@'\n",
        "    sub_web = 'http'\n",
        "    for id in soup2.find_all(id=\"tabs-6\"):\n",
        "      for td in id.find_all('td'):\n",
        "        array.append(str(td.text))\n",
        "        email = [c for c in array if sub_mail in c]\n",
        "        website = [d for d in array if sub_web in d]  \n",
        "\n",
        "    #Input Email dan Website\n",
        "    data_fix.append(email[0])\n",
        "    data_fix.append(website[0])\n",
        "\n",
        "  except:\n",
        "    error_log.append(b)\n",
        "    data_fix.append('Email Tidak Ditemukan')\n",
        "    data_fix.append('Website Tidak Ditemukan')\n",
        "\n",
        "\n",
        "\n",
        "  #write data to prepare exporting\n",
        "  data_export.append(data_fix)\n",
        "\n",
        "#Cek data akhir (opsional)\n",
        "print(data_fix)\n",
        "print(data_export)\n",
        "\n",
        "\n",
        "#Write down data to csv\n",
        "header = ['Nama Sekolah','Kepala Sekolah','Operator','Jumlah Siswa Kelas 10','Jumlah Siswa Kelas 11', 'Jumlah Siswa Kelas 12', 'Total Kelas 10', 'Total Kelas 11', 'Total Kelas 12', 'Jumlah Siswa Laki-laki', 'Jumlah Siswa Perempuan', 'Data Semester', 'Akreditasi','Alamat','Email','Website']\n",
        "with open(\"data_scrapping.csv\",\"w\",newline=\"\") as files:\n",
        "  writer = csv.writer(files)\n",
        "  writer.writerow(header)\n",
        "  writer.writerows(data_export)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}